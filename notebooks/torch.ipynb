{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecedbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embedor import *\n",
    "from src.data.data import *\n",
    "from src.plotting import *\n",
    "from src.models.mlp import *\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f76042",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, mnist_labels = get_mnist_data(n_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedor = EmbedORFast()\n",
    "emb = embedor.fit_transform(X)\n",
    "plot_graph_2D(emb, embedor.G, edge_width=0, node_color=mnist_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = X.shape[1]\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# nn = MLP(\n",
    "#     input_dim,\n",
    "#     hidden_dim=50,\n",
    "#     output_dim=2,\n",
    "#     num_layers=2).to(device)\n",
    "\n",
    "emb = torch.nn.Parameter(torch.tensor(embedor.spectral_init, dtype=torch.float, requires_grad=True).to(device))\n",
    "\n",
    "subsample_indices = torch.tensor(embedor.subsample_indices).to(device)\n",
    "epochs_per_pair_positive = torch.tensor(embedor.epochs_per_pair_positive).to(device)\n",
    "epochs_per_pair_negative = torch.tensor(embedor.epochs_per_pair_negative).to(device)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam([emb], lr=1e-2)\n",
    "# nn.train()\n",
    "n_epochs = embedor.epochs\n",
    "next_epoch_positive = torch.tensor(epochs_per_pair_positive).to(device)\n",
    "next_epoch_negative = torch.tensor(epochs_per_pair_negative).to(device)\n",
    "losses = []\n",
    "for iter in range(250):\n",
    "    optimizer.zero_grad()\n",
    "    # get pairs for this epoch\n",
    "    idx_positive = torch.where(next_epoch_positive <= iter)[0]\n",
    "    idx_negative = torch.where(next_epoch_negative <= iter)[0]\n",
    "    if len(idx_positive) == 0 and len(idx_negative) == 0:\n",
    "        continue\n",
    "    # update next epoch\n",
    "    next_epoch_positive[idx_positive] += epochs_per_pair_positive[idx_positive]\n",
    "    next_epoch_negative[idx_negative] += epochs_per_pair_negative[idx_negative]\n",
    "    # get pairs\n",
    "    pairs_positive = subsample_indices[:, idx_positive]\n",
    "    pairs_negative = subsample_indices[:, idx_negative]\n",
    "    # stack to get batch\n",
    "    # src_embeddings_positive = nn(X[pairs_positive[0]])\n",
    "    # dst_embeddings_positive = nn(X[pairs_positive[1]])\n",
    "    # src_embeddings_negative = nn(X[pairs_negative[0]])\n",
    "    # dst_embeddings_negative = nn(X[pairs_negative[1]])\n",
    "    src_embeddings_positive = emb[pairs_positive[0]]\n",
    "    dst_embeddings_positive = emb[pairs_positive[1]]\n",
    "    src_embeddings_negative = emb[pairs_negative[0]]\n",
    "    dst_embeddings_negative = emb[pairs_negative[1]]\n",
    "\n",
    "    f_positive = torch.pow(1 + torch.norm(src_embeddings_positive - dst_embeddings_positive, dim=1, p=2), -1)\n",
    "    f_negative = torch.pow(1 + torch.norm(src_embeddings_negative - dst_embeddings_negative, dim=1, p=2), -1)\n",
    "\n",
    "\n",
    "    ## TODO: make sure no pairs (i,j) are sampled s.t. i == j!! \n",
    "    loss_positive = -torch.log(f_positive)\n",
    "    loss_negative = -torch.log(1 - f_negative) * embedor.gamma\n",
    "    loss = loss_positive.sum() + loss_negative.sum()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"iter:{iter}, loss:{loss.item()}\")\n",
    "    losses.append(loss.item())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248071d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings = emb\n",
    "plot_graph_2D(final_embeddings.detach().cpu().numpy(), embedor.G, edge_width=0, node_color=mnist_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec4338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isorc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
